{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG7HbXjDbWIk",
        "outputId": "8a310700-3130-4110-e704-ca088f74ebdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from kaggle) (2.28.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from kaggle) (1.26.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from requests->kaggle) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: colorama in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from tqdm->kaggle) (0.4.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b32WexQtbWxM",
        "outputId": "39b02658-9255-4f3d-f001-5ff439561465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (3.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from pyspark) (0.10.9.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3gr1pD3bY5I",
        "outputId": "4f92dc36-04cf-4c57-aa7a-1f7ba0523155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from click->nltk) (0.4.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuWtxy6gr-s5",
        "outputId": "954cb541-e3c4-4a6e-eac9-4d68eae68ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (1.23.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIy70sDsr-s7",
        "outputId": "56a9b9b0-2168-4b4b-820a-fc78a1297a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (1.4.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from pandas) (1.23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Th1cA7GbbVY",
        "outputId": "d46a2a4d-e1db-459e-c10e-6a6e290f9752"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tUiPZkOnbeEY"
      },
      "outputs": [],
      "source": [
        "#!mkdir -p ~/.kaggle\n",
        "#!cp /content/drive/MyDrive/A_Progetti/AlgoForMassiveDatasets/kaggle.json ~/.kaggle\n",
        "#!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7mVwO8fbhtM",
        "outputId": "477c0272-ea04-4f41-9df4-eacb01285a6d"
      },
      "outputs": [],
      "source": [
        "#!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PcSKMiabQfo"
      },
      "source": [
        "# Preprocessing\n",
        "We're going to preprocess the original dataset from Kaggle to reduce its size and only work on meaningful data for our analysis: \n",
        "1. unzip the provided file to work on individual CSV files;\n",
        "2. filter only English written tweets to build a coherent language base;\n",
        "3. remove useless columns such as the account description or the number of retweets;\n",
        "4. possibly remove some stop-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FGloinN5cB-0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-nk-h98bQfo"
      },
      "source": [
        "## Code imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QKJIbm2HbQfo"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pyspark\n",
        "import regex\n",
        "import shutil\n",
        "import gzip\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.functions import concat_ws\n",
        "import multiprocessing\n",
        "import pandas\n",
        "from typing import List\n",
        "from datetime import datetime\n",
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "\n",
        "\n",
        "KAGGLE_DATASET_DIRECTORY = \"out/ukraine-russian-crisis-twitter-dataset-1-2-m-rows\"\n",
        "KAGGLE_DATASET = \"ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip\"\n",
        "WORKERS_CORES = multiprocessing.cpu_count()\n",
        "FILTER_LANGUAGE = \"en\"\n",
        "\n",
        "TOP_HASHTAGS = ['ukraine', 'russia', 'standwithukraine', 'putin', 'russian', 'mariupol', 'news', 'azovstal', 'ukrainerussianwar', 'ukrainian', 'nato', 'ukrainewar', 'ukraine', 'business', 'ukrainerussiawar', 'kharkiv', 'usa', 'tigray', 'armukrainenow', 'kyiv', 'russianukrainianwar', 'stoprussia', 'slavaukraini', 'standwithukraine', 'stopputinnow', 'stopputin', 'war', 'russiaukrainewar', 'savemariupol', 'zelenskyy', 'anonymous', 'biden', 'russians', 'eu', 'ukraineunderattack', 'kherson', 'standwithukriane', 'scotus', 'helpukraine', 'russianwarcrimes', 'tigraygenocide', 'us', 'nft', 'china', 'zelenskiynft', 'zelensky', 'endtigraysiege', 'safeairliftukraine', 'azov', 'giveaway', 'europe', 'ukraineunderattack', 'oprussia', 'donbass', 'canada', 'bucha', 'moscow', 'nfts', 'syria', 'poland', 'uk', 'russiaukraine', 'breaking', 'india', 'odesa', 'germany', 'ukrainians', 'russianarmy', 'donetsk', 'putinwarcrimes', 'mykolaiv', 'ukrainerussia', 'belarus', 'warcrimes', 'kiev', 'healthy', 'healthyeating', 'donbas', 'ukraineinvasion', 'breakingnews', 'roevwade', 'united24', 'luhansk', 'peace', 'stoprussianaggression', 'supportukraine', 'severodonetsk', 'putinwarcriminal', 'odessa', 'crypto', 'irpin', 'bitcoin', 'noflyzone', 'moskva', 'trump', 'closethesky', 'product', 'freeshipping', 'russiaukraineconflict', 'warinukraine']\n",
        "\n",
        "\n",
        "def update_top_hashtags():\n",
        "    global TOP_HASHTAGS_INDEX, TOP_HASHTAGS_REVERSE_INDEX\n",
        "    zipped_hashtags = list(zip(TOP_HASHTAGS, range(len(TOP_HASHTAGS))))\n",
        "    TOP_HASHTAGS_INDEX = {key: value for key, value in zipped_hashtags}\n",
        "    TOP_HASHTAGS_REVERSE_INDEX = {value: key for key, value in zipped_hashtags}\n",
        "\n",
        "TOP_HASHTAGS_INDEX = dict()\n",
        "TOP_HASHTAGS_REVERSE_INDEX = dict()\n",
        "update_top_hashtags()\n",
        "\n",
        "TOP_HASHTAGS_INDEX_FILENAME = \"top_hashtags_index.json\"\n",
        "TOP_HASHTAGS_REVERSE_INDEX_FILENAME = \"top_hashtags_reverse_index.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "DO_PREPROCESS = True\n",
        "FIND_MOST_COMMON = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3JdK-o4dNyY"
      },
      "source": [
        "## Stopwords list retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rjRKq3D-dQyk"
      },
      "outputs": [],
      "source": [
        "def update_nltk():\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dqpJE0hbQfo"
      },
      "source": [
        "## Dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kmXEZixNbQfo"
      },
      "outputs": [],
      "source": [
        "def dataset_extraction(archive: str, output_directory: str) -> List[str]:\n",
        "    new_files = list()\n",
        "    with zipfile.ZipFile(archive, \"r\") as zip_ref:\n",
        "\n",
        "        return [os.path.join(output_directory, csv_name) for csv_name, _ in [os.path.splitext(g_name) for g_name in zip_ref.namelist()[:2]]]\n",
        "\n",
        "        if not os.path.isdir(output_directory):\n",
        "            zip_ref.extractall(output_directory)\n",
        "            #new_files = zip_ref.namelist()\n",
        "        # else:\n",
        "        for gzip_name in zip_ref.namelist():\n",
        "            csv_name, extension = os.path.splitext(gzip_name)\n",
        "            if os.path.isfile(os.path.join(output_directory, csv_name)):\n",
        "                continue\n",
        "            if not os.path.isfile(os.path.join(output_directory, gzip_name)):\n",
        "                zip_ref.extract(gzip_name, path=output_directory)\n",
        "            csv_path = os.path.join(output_directory, csv_name)\n",
        "            with gzip.open(os.path.join(output_directory, gzip_name), 'r') as gzip_file, open(csv_path, 'wb') as csv_file:\n",
        "                shutil.copyfileobj(gzip_file, csv_file)\n",
        "            new_files.append(csv_path)\n",
        "    return new_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38HBJGytbQfo"
      },
      "source": [
        "## Spark initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "yG9S6yf7bQfo"
      },
      "outputs": [],
      "source": [
        "def init_spark():\n",
        "    print(f\"Available CPU cores/workers: {WORKERS_CORES}\")\n",
        "    print(\"Initializing spark...\")\n",
        "    spark = (\n",
        "        pyspark.sql.SparkSession.builder\n",
        "        .master(f\"local[{WORKERS_CORES}]\")\n",
        "        .appName(\"Sparkiodi\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    spark.sparkContext.setLogLevel(\"OFF\")\n",
        "    return spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxVWcA51bQfo"
      },
      "source": [
        "## Dataset reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ziq-GV6tbQfo"
      },
      "outputs": [],
      "source": [
        "def read_dataframe(path: List[str], spark, header: bool = True, language: str = None):\n",
        "    print(\"Reading all the CSVs...\")\n",
        "    if header:\n",
        "        starting_df = (\n",
        "            spark\n",
        "            .read\n",
        "            .option(\"header\", True)\n",
        "            .option(\"multiLine\", True)\n",
        "            .csv(path)\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/*.csv\")  # Read all files\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/UkraineCombinedTweetsDeduped_FEB28_part2.csv\")    # Read the smallest file\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/0505_to_0507_UkraineCombinedTweetsDeduped.csv\")  # Read the biggest file\n",
        "        )\n",
        "    else:\n",
        "        starting_df = (\n",
        "            spark\n",
        "            .read\n",
        "            .option(\"header\", False)\n",
        "            .option(\"multiLine\", True)\n",
        "            .csv(path)\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/*.csv\")  # Read all files\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/UkraineCombinedTweetsDeduped_FEB28_part2.csv\")    # Read the smallest file\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/0505_to_0507_UkraineCombinedTweetsDeduped.csv\")  # Read the biggest file\n",
        "        )\n",
        "\n",
        "    if language:\n",
        "        broadcast_language = spark.sparkContext.broadcast(FILTER_LANGUAGE)\n",
        "        starting_df = starting_df.where(\n",
        "            starting_df.language == broadcast_language.value)\n",
        "\n",
        "    return starting_df.select(\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DArgu1bQfo"
      },
      "source": [
        "### Entities removal and hashtags extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hpABLRDBbQfo"
      },
      "outputs": [],
      "source": [
        "hashtag_regex_str = r\"(?:\\#+)([\\w_]+)\"  # hashtags\n",
        "regex_str = [\n",
        "    (r'(?:@[\\w_]+)', ''),  # @-mentions\n",
        "    (r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', ''), # URLs\n",
        "    (r'[^\\w\\s]', ' '),  # punctuation\n",
        "    (r'\\s+', ' ')  # whitespaces\n",
        "]\n",
        "\n",
        "hashtag_regex = regex.compile(hashtag_regex_str)\n",
        "regex = [(hashtag_regex, '')] + [(regex.compile(compiled[0]), compiled[1])\n",
        "                                 for compiled in regex_str]   # Keep hashtag_regex as the first applied regex\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "\n",
        "    for reg in regex:\n",
        "        text = reg[0].sub(reg[1], text)\n",
        "\n",
        "    text_list = set([\n",
        "        word.rstrip() for word in word_tokenize(text)\n",
        "        if word not in string.punctuation\n",
        "    ])\n",
        "\n",
        "    return list(text_list)\n",
        "\n",
        "\n",
        "def from_indices_to_boolean_vector(indices):\n",
        "    pads = [0 for _ in range(len(TOP_HASHTAGS))]\n",
        "    for index in indices:\n",
        "        pads[index] = 1\n",
        "    return pads\n",
        "\n",
        "\n",
        "def padding_hashtags(hashtags, indexes, compress=False):\n",
        "    if compress:\n",
        "        pads = list()\n",
        "        for tag in hashtags:\n",
        "          if tag in indexes:\n",
        "              pads.append(indexes[tag])\n",
        "        pads.sort()\n",
        "    else:\n",
        "        pads = [0 for _ in range(len(indexes))]\n",
        "        for tag in hashtags:\n",
        "            if tag in indexes:\n",
        "                pads[indexes[tag]] = 1\n",
        "    return pads\n",
        "\n",
        "\n",
        "def clean_dataframe(df, spark):\n",
        "    remover = StopWordsRemover(stopWords=stopwords.words('english'))\n",
        "    remover.setInputCol(\"tweet\")\n",
        "    remover.setOutputCol(\"filtered_tweet\")\n",
        "\n",
        "    tweets_hashtags_rdd = (\n",
        "        df.rdd\n",
        "        .map(lambda row: (clean_text(row.text), [ht.lower() for ht in hashtag_regex.findall(row.text)]))\n",
        "    )\n",
        "\n",
        "    if FIND_MOST_COMMON:\n",
        "        global TOP_HASHTAGS\n",
        "        TOP_HASHTAGS = (\n",
        "            tweets_hashtags_rdd\n",
        "            .flatMap(lambda row: row[1])\n",
        "            .map(lambda row: (row, 1))\n",
        "            .reduceByKey(lambda x, y: x + y)\n",
        "            .map(lambda row: (row[1], row[0]))\n",
        "            .sortByKey(ascending=False)\n",
        "            .map(lambda row: row[1])\n",
        "            .take(100)\n",
        "        )\n",
        "        update_top_hashtags()\n",
        "\n",
        "    broadcast_top_hashtags_index = spark.sparkContext.broadcast(TOP_HASHTAGS_INDEX)\n",
        "        \n",
        "    df = (\n",
        "        tweets_hashtags_rdd\n",
        "        .map(lambda row: (row[0], padding_hashtags(row[1], broadcast_top_hashtags_index.value, True)))\n",
        "        .toDF([\"tweet\", \"hashtags\"])\n",
        "    )\n",
        "    df = (\n",
        "        remover\n",
        "        .transform(df)\n",
        "        .select(\"filtered_tweet\", \"hashtags\")\n",
        "        .withColumn(\"filtered_tweet\", concat_ws(\" \", \"filtered_tweet\"))\n",
        "        .withColumn(\"hashtags\", concat_ws(\" \", \"hashtags\"))\n",
        "    )\n",
        "    df = df.where((df.filtered_tweet != \"\"))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH7ZHI4ObQfo"
      },
      "source": [
        "## Preprocessed dataset storing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "D0Hn0pWabQfo"
      },
      "outputs": [],
      "source": [
        "def write_preprocessed(df, output_directory: str) -> str:\n",
        "    print(\"Writing top hashtags...\")\n",
        "    with open(TOP_HASHTAGS_INDEX_FILENAME, \"w\") as fp:\n",
        "        json.dump(TOP_HASHTAGS_INDEX, fp)\n",
        "    with open(TOP_HASHTAGS_REVERSE_INDEX_FILENAME, \"w\") as fp:\n",
        "        json.dump(TOP_HASHTAGS_REVERSE_INDEX, fp)\n",
        "    print(\"Writing all the CSVs...\")\n",
        "    (df\n",
        "        # .coalesce(WORKERS_CORES)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .option(\"header\", False)\n",
        "        .csv(output_directory)\n",
        "     )\n",
        "\n",
        "    for dirpath, dirnames, filenames in os.walk(output_directory):\n",
        "        for filename in filenames:\n",
        "            name, extension = os.path.splitext(filename)\n",
        "            if extension == \".csv\":\n",
        "                continue\n",
        "            filepath = os.path.join(dirpath, filename)\n",
        "            try:\n",
        "                os.remove(filepath)\n",
        "            except:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ldEu38RibQfo"
      },
      "outputs": [],
      "source": [
        "def compress(path: str) -> str:\n",
        "    print(\"Creating archive directory...\")\n",
        "    if os.path.isfile(path):\n",
        "        name, ext = os.path.splitext(path)\n",
        "        output_archive = f\"{name}.zip\"\n",
        "        with zipfile.ZipFile(output_archive, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "            zf.write(path, os.path.basename(path))\n",
        "    else:\n",
        "        output_archive = shutil.make_archive(path, 'zip', path)\n",
        "    return output_archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "y7Q9_1C7bQfo"
      },
      "outputs": [],
      "source": [
        "def create_single_csv(folders: List[str], overwrite: bool = False) -> str:\n",
        "    single_filename = 'out/dataset.csv'\n",
        "    filemode = 'wb' if overwrite else 'ab'\n",
        "    with open(single_filename, filemode) as output_file:\n",
        "        for directory in folders:\n",
        "            for dirpath, dirnames, filenames in os.walk(directory):\n",
        "                for filename in filenames:\n",
        "                    name, extension = os.path.splitext(filename)\n",
        "                    if extension != \".csv\":\n",
        "                        continue\n",
        "                    filepath = os.path.join(dirpath, filename)\n",
        "                    with open(filepath, 'rb') as csv:\n",
        "                        shutil.copyfileobj(csv, output_file)\n",
        "    return single_filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAT0ui32bQfo"
      },
      "source": [
        "## Start preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqFI4HprbQfo",
        "outputId": "d5cfe8ce-0d87-4e99-a97b-1f39a3ba2b6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Lion\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Lion\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available CPU cores/workers: 4\n",
            "Initializing spark...\n",
            "Reading all the CSVs...\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 4) (ELROND.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 19 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\preprocess.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000027?line=14'>15</a>\u001b[0m     compress(csv)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000027?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m DO_PREPROCESS:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000027?line=17'>18</a>\u001b[0m     preprocess()\n",
            "\u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\preprocess.ipynb Cell 30'\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000027?line=8'>9</a>\u001b[0m     spark \u001b[39m=\u001b[39m init_spark()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000027?line=9'>10</a>\u001b[0m df \u001b[39m=\u001b[39m read_dataframe(new_files, spark, language\u001b[39m=\u001b[39mFILTER_LANGUAGE)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000027?line=10'>11</a>\u001b[0m df \u001b[39m=\u001b[39m clean_dataframe(df, spark)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000027?line=11'>12</a>\u001b[0m output_directory \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mout/preprocessed_\u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000027?line=12'>13</a>\u001b[0m write_preprocessed(df, output_directory)\n",
            "\u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\preprocess.ipynb Cell 24'\u001b[0m in \u001b[0;36mclean_dataframe\u001b[1;34m(df, spark)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=66'>67</a>\u001b[0m \u001b[39mif\u001b[39;00m FIND_MOST_COMMON:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=67'>68</a>\u001b[0m     \u001b[39mglobal\u001b[39;00m TOP_HASHTAGS\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=68'>69</a>\u001b[0m     TOP_HASHTAGS \u001b[39m=\u001b[39m (\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=69'>70</a>\u001b[0m         tweets_hashtags_rdd\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=70'>71</a>\u001b[0m         \u001b[39m.\u001b[39;49mflatMap(\u001b[39mlambda\u001b[39;49;00m row: row[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=71'>72</a>\u001b[0m         \u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m row: (row, \u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=72'>73</a>\u001b[0m         \u001b[39m.\u001b[39;49mreduceByKey(\u001b[39mlambda\u001b[39;49;00m x, y: x \u001b[39m+\u001b[39;49m y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=73'>74</a>\u001b[0m         \u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m row: (row[\u001b[39m1\u001b[39;49m], row[\u001b[39m0\u001b[39;49m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=74'>75</a>\u001b[0m         \u001b[39m.\u001b[39;49msortByKey(ascending\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=75'>76</a>\u001b[0m         \u001b[39m.\u001b[39mtake(\u001b[39m100\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=76'>77</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=77'>78</a>\u001b[0m     update_top_hashtags()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lion/source/progetto_malchiodi/preprocess.ipynb#ch0000021?line=79'>80</a>\u001b[0m broadcast_top_hashtags_index \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39mbroadcast(TOP_HASHTAGS_INDEX)\n",
            "File \u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages\\pyspark\\rdd.py:995\u001b[0m, in \u001b[0;36mRDD.sortByKey\u001b[1;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[0;32m    990\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapPartitions(sortPartition, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    992\u001b[0m \u001b[39m# first compute the boundary of each part via sampling: we want to partition\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[39m# the key-space into bins such that the bins have roughly the same\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[39m# number of (key, value) pairs falling into them\u001b[39;00m\n\u001b[1;32m--> 995\u001b[0m rddSize \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcount()\n\u001b[0;32m    996\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m rddSize:\n\u001b[0;32m    997\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m  \u001b[39m# empty RDD\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages\\pyspark\\rdd.py:1521\u001b[0m, in \u001b[0;36mRDD.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m   1513\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[0;32m   1520\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1521\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
            "File \u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages\\pyspark\\rdd.py:1508\u001b[0m, in \u001b[0;36mRDD.sum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1499\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[NumberOrArray]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNumberOrArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1500\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   1509\u001b[0m         \u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd\n\u001b[0;32m   1510\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages\\pyspark\\rdd.py:1336\u001b[0m, in \u001b[0;36mRDD.fold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[0;32m   1333\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[1;32m-> 1336\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[0;32m   1337\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
            "File \u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages\\pyspark\\rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[0;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[0;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
            "File \u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Users\\Lion\\source\\progetto_malchiodi\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 4) (ELROND.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 19 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 19 more\r\n"
          ]
        }
      ],
      "source": [
        "def preprocess() -> None:\n",
        "    new_files = dataset_extraction(KAGGLE_DATASET, KAGGLE_DATASET_DIRECTORY)\n",
        "    if not len(new_files):\n",
        "        print(\"No new files to process\")\n",
        "        return\n",
        "\n",
        "    update_nltk()\n",
        "    if not ('spark' in locals() or 'spark' in globals()):\n",
        "        spark = init_spark()\n",
        "    df = read_dataframe(new_files, spark, language=FILTER_LANGUAGE)\n",
        "    df = clean_dataframe(df, spark)\n",
        "    output_directory = f\"out/preprocessed_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
        "    write_preprocessed(df, output_directory)\n",
        "    csv = create_single_csv([output_directory])\n",
        "    compress(csv)\n",
        "\n",
        "if DO_PREPROCESS:\n",
        "    preprocess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfPWW6_UcGU8",
        "outputId": "d0873693-2cf9-4092-c426-2a7427642f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start time: 1656665654.5339313\n",
            "End time: 1656665806.4315674\n",
            "Elapsed time: 00:02:31\n"
          ]
        }
      ],
      "source": [
        "end_time = time.time()\n",
        "print(f\"Start time: {start_time}\")\n",
        "print(f\"End time: {end_time}\")\n",
        "print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(end_time - start_time))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T13A7gZM6IGS",
        "outputId": "803ef720-ceb8-433a-91b1-c626342c1a81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "still europe must unacceptable invasion prime sanctions reaching go unjustified spoke meet need moment minister,1\n",
            "ukrainian gave flew life first force super air jets fighter pilot natalia woman 28 ukraine true,1\n",
            " latest try saturday buses russian air conducting denied soil carried residents mariupol kyiv cross strike devastated war red,1\n",
            "game x unit ambush changer ddd military army,1 2\n",
            "beginning 16 russian five invasion since entry based passed examine mercenaries sydney 1 weeks article conflict today recent morning herald,1\n",
            "14 killed russian boy old yura bikes front missed wanted told year miracle survived rode humanitarian father shoot soldier pick aid,5 56\n",
            "saw 3 wrong months ago video,1 2 9 16 20 21 75 89\n",
            "elegant face lettering read picked ignorance slavery freedom three white slogans possible strength peace winston stood party war,1\n",
            "applause room erupted thunderous front falls dakota south 3000 tonight sioux badass people said,23\n",
            "stuff funny back,1\n",
            "-rw-r--r-- 1 root root 50818995 Jul  1 08:56 out/dataset.csv\n"
          ]
        }
      ],
      "source": [
        "!tail out/dataset.csv\n",
        "!ls -l out/dataset.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocess.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c1b9ca2a5b5570164d24b37b8802acc778bb5029148cbff343ca7a6716f74867"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
